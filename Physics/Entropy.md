# Entropy

**the measure of a system's thermal energy per unit temperature that is unavailable for doing useful work**. Because work is obtained from ordered molecular motion, the amount of  entropy is also a measure of the molecular disorder, or randomness, of a system: *the higher the degree of randomness, the higher the entropy*.

Examples: Gas is a bottle is highly compressed. Thus, each molecule can only move slightly. The number of possible states is small(er). On the other hand, the same gas in a room will spread out evenly. Each molecule has many different paths to go to. Thus, there are many possible states. The entropy is high.

Entropy is central to the **second law of thermodynamics**, which states that the entropy of an isolated system left to spontaneous evolution cannot decrease with time (_ordering something requires energy_). As a result, isolated systems evolve toward thermodynamic equilibrium, where the entropy is highest (_gas molecules released from a balloon spread out evenly in a room_). A consequence of the second law of thermodynamics is that certain processes are irreversible (_the released gas needs to be forced back into the balloon, requiring a log of energy_).

## Information Technology

In IT entropy describes the amount of uncertainty or surprise: The higher the randomness/variation of data, the higher the entropy. A random number generator has infinite entropy, while a repeating loop of continuous 1s has no entropy. 